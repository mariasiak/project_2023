# -*- coding: utf-8 -*-
"""nn2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GrCePp2wNz_txupM4EyJYjtS9B13Y0kd

# Imports
"""

!pip install idx2numpy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import keras
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Reshape
from keras.optimizers import RMSprop

from tensorflow.keras.layers import BatchNormalization

from sklearn.model_selection import train_test_split
import idx2numpy

"""# Data Read and Check"""

file_path = 'reduce.config'

# extract variables from the file
variables = {}

file = open(file_path, 'r')

for line in file:
    key, value = line.strip().split(':')
    variables[key.strip()] = value.strip()

# access the values for the variables
dataset = variables.get('dataset')
queryset = variables.get('queryset')
output_dataset_file = variables.get('output_dataset_file')
output_query_file = variables.get('output_query_file')

# set data and query paths
data_path = dataset
query_path = queryset

# load data and query from paths
data = idx2numpy.convert_from_file(data_path)
query = idx2numpy.convert_from_file(query_path)

# display the reshaped image
plt.imshow(data[120], cmap='gray')
plt.title("Example Image")
plt.show()

# shape of data set
print("Data set shape: ",data.shape)

# shape of query set
print("Query set shape: ",query.shape)

"""# Data Preproccessing

Convert from 28x28 to 28x28x1
"""

# underscored data and query hold the postproccessed data, and query sets
_data = data.reshape(-1, 28,28, 1)
_query = query.reshape(-1, 28,28, 1)

# shape of data set
print("Data set shape: ",_data.shape)

# shape of query set
print("Query set shape: ",_query.shape)

"""Change data type to float32"""

# change the data type of _data and _query to float32
_data = _data.astype('float32')
_query = _query.astype('float32')

_data.dtype, _query.dtype

"""Rescale values from [0, 255] to [0.0, 1.0]"""

np.max(_data), np.max(_query)

_data = _data / np.max(_data)
_query = _query / np.max(_query)

np.max(_data), np.max(_query)

"""Split the data into train, valid, ground"""

train_X,valid_X,train_ground,valid_ground = train_test_split(_data,
                                                             _data,
                                                             test_size=0.181,
                                                             random_state=333)

"""# Neural Network"""

# hyperparameters
batch_size = 128
epochs = 12

# input layer
inChannel = 1
x, y = 28, 28
input_img = Input(shape=(x, y, inChannel))

conv_shape = (3, 3)
filters = (64, 32, 16)

x = Conv2D(filters[0], conv_shape, activation='relu', padding='same')(input_img)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2), padding='same')(x)

x = Conv2D(filters[1], conv_shape, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2), padding='same')(x)

x = Conv2D(filters[2], conv_shape, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2), padding='same')(x)

# flatten
x = Flatten()(x)

# latent space
x = Dense(10, activation='sigmoid')(x)
x = BatchNormalization()(x)

# decoder
x = Dense(4 * 4 * filters[2], activation='relu')(x)
x = Reshape((4, 4, filters[2]))(x)

x = Conv2D(filters[2], conv_shape, activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)

x = Conv2D(filters[1], conv_shape, activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)

x = Conv2D(filters[0], conv_shape, activation='relu')(x)
x = UpSampling2D((2, 2))(x)

# output layer
decoded = Conv2D(1, conv_shape, activation='sigmoid', padding='same')(x)

# autoencoder model
autoencoder = Model(input_img, decoded)
autoencoder.compile(loss='mean_squared_error', optimizer=RMSprop())

autoencoder.summary()

autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))

loss = autoencoder_train.history['loss']
val_loss = autoencoder_train.history['val_loss']
epochs = range(epochs)
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

pred = autoencoder.predict(_query)

pred.shape

plt.figure(figsize=(20, 4))
print("Test Images")
for i in range(10):
    plt.subplot(2, 10, i+1)
    plt.imshow(_query[i, ..., 0], cmap='gray')
plt.show()
plt.figure(figsize=(20, 4))
print("Reconstruction of Test Images")
for i in range(10):
    plt.subplot(2, 10, i+1)
    plt.imshow(pred[i, ..., 0], cmap='gray')
plt.show()

"""# Create Output In Latent Space"""

encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('dense').output)

query_bottleneck_output = encoder_model.predict(_query)

query_bottleneck_output.shape

query_bottleneck_output*=255
query_bottleneck_output=query_bottleneck_output.astype(int)
query_bottleneck_output[0]

data_bottleneck_output = encoder_model.predict(_data)

data_bottleneck_output.shape

data_bottleneck_output*=255
data_bottleneck_output=data_bottleneck_output.astype(int)
data_bottleneck_output[0]

data_bottleneck_output[0]

np.savetxt(output_dataset_file, data_bottleneck_output, delimiter=',', fmt='%d')
np.savetxt(output_query_file, query_bottleneck_output, delimiter=',', fmt='%d')